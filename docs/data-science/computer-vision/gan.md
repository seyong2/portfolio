---
title: Real or Fake? GAN Explained
parent: Computer Vision
nav_order: 4
layout: default
---

## What are GANs?

A **Generative Adversarial Network (GAN)** is a machine learning model composed of two neural networks: a **generator** and a **discriminator**. As the name suggests, these networks engage in an adversarial game where one's success comes at the expense of the other. The generator attempts to deceive the discriminator by producing realistic outputs, while the discriminator strives to correctly distinguish between real and generated (fake) data.

For example, imagine the generator is trying to create fake diamonds, and the discriminator is tasked with identifying whether a diamond is real or fake. Initially, the generator produces poor imitations that the discriminator can easily spot. However, as training progresses, the generator learns from the discriminator’s feedback and improves its outputs. Eventually, the generator may become so good that the discriminator struggles to tell real and fake diamonds apart—sometimes even guessing.

The overall GAN architecture can be illustrated in the following figure:

<p align="center">
  <img src="https://github.com/user-attachments/assets/a165be33-10f6-4135-9474-dfcaffda9a60">
</p>

Relating this to our diamond example:

- $$Z$$ represents the raw material or input noise the generator uses to create a fake diamond.
- $$G(Z)$$ is the diamond produced by the generator.
- The discriminator examines $$G(Z)$$ and determines whether it is real or fake.
- A real diamond is denoted by $$Z$$.

### Objective Functions

Since a GAN is composed of two neural networks in an adversarial relationship, it requires two objective functions—one for each network. Fortunately, these can be formulated using **Binary Cross Entropy**. Let’s first examine the loss function for the discriminator.

#### Discriminator

The objective function for the discriminator is defined as:

$$ max \frac{1}{m} \sum_{i=1}^{m} [log D(x^{(i)}) + log(1 - D(G(z^{(i)})))] $$

where $$m$$ is the number of training samples. Since the discriminator outputs the probability that a given input is real (as opposed to generated), both $$ D(x^{(i)}) $$ and $$ D(G(z^{(i)})) $$ are values between 0 and 1. 

The goal of the discriminator is to assign **high probabilities to real data** (i.e., $$ D(x^{(i)}) \sim 1$$) and **low probabilities to fake data generated by the generator** (i.e., $$ D(G(z^{(i)}))\sim 0 $$), thereby maximizing the objective function.

#### Generator

For the generator, we focus only on the second term of the discriminator's objective, since it involves the generator's output:

$$ min \frac{1}{m} \sum_{i=1}^{m} log(1 - D(G(z^{(i)}))) $$

Here, the generator aims to fool the discriminator by making $$ G(z^{(i)}) $$ as close to 1 as possible (i.e., being classified as real). However, this formulation can lead to the **vanishing grandient problem**, especially when $$ D(G(z^{(i)})) $$ is close to 0.

<p align="center">
  <img src="https://github.com/user-attachments/assets/2239cecd-5965-4cd9-b9d5-2290cff4d218">
</p>

As shown in the figure above, the grandients of $$ log (1-D) $$ become very small when $$D$$ is near 1, making it difficult for the generator to learn. 

To address this, the generator's objective function is often modified to:

$$ min \frac{1}{m} \sum_{i=1}^{m} -log(D(G(z^{(i)}))) $$

which is mathematically equivalent to:

$$ max \frac{1}{m} \sum_{i=1}^{m} log(D(G(z^{(i)}))) $$

The adjusted formulation provides **stronger gradients**, espeically in the early stages of training, and helps the generator learn more effectively.

#### Resources
- [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661)
